{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from heapq import nlargest\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import HiveContext\n",
    "from collections import defaultdict\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import CountVectorizer, IDF, Tokenizer\n",
    "from pyspark.mllib.linalg import Vectors, SparseVector, DenseVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 1\n",
    "# def read_line(line):\n",
    "#     if line is None:\n",
    "#         return None\n",
    "#     line = line.split('\\t')[-1]\n",
    "#     line = re.sub('[^A-Za-z_]', '', line)\n",
    "#     line = re.sub(' +', ' ', line.replace('_', ' '))\n",
    "#     return line.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 1\n",
    "# def read_data(sc, data_path):\n",
    "#     if sc is None or data_path is None:\n",
    "#         return None\n",
    "#     documents = sc.textFile(data_path).map(lambda line : read_line(line))\n",
    "#     schema = StructType([StructField(\"activity\", StringType(), True)])\n",
    "#     use this if the row has only one column\n",
    "#     documents = documents.map (lambda x: Row(x))\n",
    "#     documents = SQLContext(sc).createDataFrame(documents, schema)\n",
    "    \n",
    "#     tokenizer = Tokenizer(inputCol=\"activity\", outputCol=\"tokens\")\n",
    "#     documents = tokenizer.transform(documents)\n",
    "\n",
    "#     remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n",
    "#     documents = remover.transform(documents)\n",
    "#     return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_line(row):\n",
    "    if row is None:\n",
    "        return None\n",
    "    row = row.split(',')[0]\n",
    "    row_formatted = re.sub('[^A-Za-z_]', '', row)\n",
    "    row_formatted = re.sub(' +', ' ', row_formatted.replace('_', ' '))\n",
    "    return row, row_formatted.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(sc, data_path):\n",
    "    if sc is None or data_path is None:\n",
    "        return None\n",
    "    documents = sc.textFile(data_path).map(lambda line : read_line(line))\n",
    "    schema = StructType([StructField(\"raw_activity\", StringType(), True),\n",
    "                         StructField(\"activity\", StringType(), True)])\n",
    "    documents = SQLContext(sc).createDataFrame(documents, schema)  \n",
    "    tokenizer = Tokenizer(inputCol=\"activity\", outputCol=\"tokens\")\n",
    "    documents = tokenizer.transform(documents)\n",
    "\n",
    "    remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n",
    "    documents = remover.transform(documents)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_transform(documents):\n",
    "    if documents is None:\n",
    "        return None\n",
    "    countVectorizer = CountVectorizer(inputCol=\"filtered_tokens\", outputCol=\"feature_counts\")\n",
    "    countVectorizerModel = countVectorizer.fit(documents)\n",
    "    documents = countVectorizerModel.transform(documents)\n",
    "    idf = IDF(inputCol=\"feature_counts\", outputCol=\"features\")\n",
    "    idfModel = idf.fit(documents)\n",
    "    documents = idfModel.transform(documents)\n",
    "    return countVectorizerModel.vocabulary, documents\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering(documents, output_path, k=2, max_iter=20):\n",
    "    if documents is None or output_path is None:\n",
    "        return None\n",
    "    output_path = os.getcwd() + \"/\" + output_path + \"/\" + str(k)\n",
    "    if os.path.exists(output_path):\n",
    "        shutil.rmtree(output_path) \n",
    "    os.makedirs(output_path)\n",
    "    out_file = open(os.path.join(output_path, \"cost.txt\"), \"w\")\n",
    "    kmeans = KMeans(featuresCol=\"features\").setK(k).setMaxIter(max_iter)\n",
    "    km_model = kmeans.fit(documents)\n",
    "    clustersTable = km_model.transform(documents)\n",
    "    clusterCenters = km_model.clusterCenters()\n",
    "    wssse = km_model.computeCost(documents)\n",
    "    out_file.write(\"Cluster {}\".format(k))\n",
    "    out_file.write(\"\\n\")\n",
    "    out_file.write(\"cost:\" + str(wssse))\n",
    "    out_file.write(\"\\n\")\n",
    "    return clustersTable, clusterCenters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(row):\n",
    "    return (row.prediction, ) + tuple(row.features.toArray().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_add(v1, v2):\n",
    "    #assert isinstance(v1, SparseVector) and isinstance(v2, SparseVector)\n",
    "    assert v1.size == v2.size\n",
    "    values = defaultdict(float) # Dictionary with default value 0.0\n",
    "    # Add values from v1\n",
    "    for i in range(v1.indices.size):\n",
    "        values[v1.indices[i]] += v1.values[i]\n",
    "    # Add values from v2\n",
    "    for i in range(v2.indices.size):\n",
    "        values[v2.indices[i]] += v2.values[i]\n",
    "    return Vectors.sparse(v1.size, dict(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_divide(v):\n",
    "    #assert isinstance(v, SparseVector) \n",
    "    values = defaultdict(float) # Dictionary with default value 0.0\n",
    "     # Add values from v[0]\n",
    "    for i in range(v[0].indices.size):\n",
    "        values[v[0].indices[i]] = v[0].values[i] / float(v[1])\n",
    "    return Vectors.sparse(v[0].size, dict(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_index(v, k):\n",
    "    #assert isinstance(v, SparseVector)\n",
    "    dct = {}\n",
    "    for i in range(v.indices.size):\n",
    "        dct[v.indices[i]] = v.values[i]\n",
    "    return nlargest(k, dct, key=dct.get)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Very Basic implementation of top k words\n",
    "# def get_top_keywords(clustersTable, vocab, output_path, n_terms=10, k=2):\n",
    "#     if clustersTable is None or vocab is None or output_path is None:\n",
    "#         return None\n",
    "#     if not os.path.exists(output_path):\n",
    "#         os.makedirs(output_path)\n",
    "#     out_path = os.getcwd() + \"/\" + output_path + \"/\" + str(k)\n",
    "#     out_file = open(os.path.join(out_path, \"out.txt\"), \"w\")\n",
    "#     clusters = clustersTable.select(\"features\",\"prediction\")\n",
    "#     clusters = clusters.rdd.map(extract).toDF([\"prediction\"]) \n",
    "#     clusters = clusters.groupby(\"prediction\").mean()\n",
    "#     clusters = clusters.drop(\"prediction\")\n",
    "#     clusters = clusters.drop(\"avg(prediction)\")\n",
    "#     clusters_array = np.array(clusters.collect())\n",
    "#     for idx, row in enumerate(clusters_array):\n",
    "#         out_file.write(\"cluster {}\".format(idx))\n",
    "#         out_file.write(\"\\n\")\n",
    "#         out_file.write(\",\".join([vocab[t] for t in np.argsort(row)[-n_terms:]]))\n",
    "#         out_file.write(\"\\n\")\n",
    "#     out_file.close()\n",
    "#     return None         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic implementation of top k words\n",
    "def get_top_keywords(clustersTable, vocab, output_path, n_terms=10, k=2):\n",
    "    if clustersTable is None or vocab is None or output_path is None:\n",
    "        return None\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "    out_path = os.getcwd() + \"/\" + output_path + \"/\" + str(k)\n",
    "    out_file = open(os.path.join(out_path, \"out.txt\"), \"w\")\n",
    "    clusters = clustersTable.select(\"prediction\", \"features\")\n",
    "    clusters_array = clusters.rdd \\\n",
    "        .mapValues(lambda v: (v.toArray(), 1)) \\\n",
    "        .reduceByKey(lambda a,b: (a[0]+b[0], a[1]+b[1])) \\\n",
    "        .mapValues(lambda v: v[0]/v[1]).collect()\n",
    "    for row in clusters_array:\n",
    "        out_file.write(\"cluster {}\".format(row[0]))\n",
    "        out_file.write(\"\\n\")\n",
    "        out_file.write(\",\".join([vocab[t] for t in np.argsort(row[1])[-n_terms:]]))\n",
    "        out_file.write(\"\\n\")\n",
    "    out_file.close()\n",
    "    return clusters_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced implementation of top k words\n",
    "def get_top_keywords_test(clustersTable, vocab, output_path, n_terms=10, k=2):\n",
    "    if clustersTable is None or vocab is None or output_path is None:\n",
    "        return None\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "    out_path = os.getcwd() + \"/\" + output_path + \"/\" + str(k)\n",
    "    out_file = open(os.path.join(out_path, \"out.txt1\"), \"w\")\n",
    "    clusters = clustersTable.select(\"prediction\", \"features\")\n",
    "    clusters_array = clusters.rdd \\\n",
    "        .mapValues(lambda v: (v, 1)) \\\n",
    "        .reduceByKey(lambda a,b: (sparse_add(a[0], b[0]), a[1]+b[1])) \\\n",
    "        .mapValues(lambda v: sparse_divide(v)) \\\n",
    "        .mapValues(lambda v: get_top_index(v, n_terms)).collect()\n",
    "    for row in clusters_array:\n",
    "        out_file.write(\"cluster {}\".format(row[0]))\n",
    "        out_file.write(\"\\n\")\n",
    "        out_file.write(\",\".join([vocab[t] for t in row[1]]))\n",
    "        out_file.write(\"\\n\")\n",
    "    out_file.close()\n",
    "    return clusters_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_clustering(sc, input_path, output_path, num_clusters_list, max_iter=20):\n",
    "    if sc is None or input_path is None or num_clusters_list is None or output_path is None:\n",
    "        return None\n",
    "    documents = read_data(sc, input_path)\n",
    "    vocab, documents = tf_idf_transform(documents)\n",
    "    for k in num_clusters_list:\n",
    "        clustersTable, clusterCenters = clustering(documents, output_path, k, max_iter)\n",
    "        write_results(sc, clustersTable, output_path, k)\n",
    "        #get_top_keywords(clustersTable, vocab, output_path, max_iter, k)\n",
    "        get_top_keywords_test(clustersTable, vocab, output_path, max_iter, k)\n",
    "    return clustersTable, clusterCenters, documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_results(sc, clustersTable, output_path, k):\n",
    "    if sc is None or clustersTable is None or output_path is None:\n",
    "        return None\n",
    "    output_path = output_path + '/' + str(k) + '/results'\n",
    "    if os.path.exists(output_path):\n",
    "        shutil.rmtree(output_path) \n",
    "    resultsdf = clustersTable.select('activity', 'prediction')\n",
    "    resultsdf.write.format('csv').option('delimiter', '\\t').option('header', 'true').save(output_path)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(conf=SparkConf().setAppName(\"tf-idf_clustering_on_spark\"))\n",
    "clustersTable, clusterCenters, documents = run_clustering(sc, 'test_data_bck', 'output', [3])\n",
    "clustersTable = clustersTable.withColumn('dist', dist(clustersTable.features, clustersTable.prediction))\n",
    "clustersTable.show(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
